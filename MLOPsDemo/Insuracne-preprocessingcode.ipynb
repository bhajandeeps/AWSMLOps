{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a36eb7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/WipCoe/MLOPsDemo\n",
      "download: s3://mlops-insurance/deploymentfiles/batchscoring-config.json to deployment/batchscoring-config.json\n",
      "download: s3://mlops-insurance/deploymentfiles/batchscoring.sh to deployment/batchscoring.sh\n",
      "download: s3://mlops-insurance/deploymentfiles/batchscoring.yml to deployment/batchscoring.yml\n",
      "download: s3://mlops-insurance/deploymentfiles/common-config.json to deployment/common-config.json\n",
      "download: s3://mlops-insurance/deploymentfiles/xgpreporcessing.py to deployment/xgpreporcessing.py\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "#! aws s3 cp s3://mlops-insurance/deploymentfiles/insurance.csv /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/batchscoring-config.json  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/batchscoring.sh  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/batchscoring.yml  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/common-config.json  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/xgpreporcessing.py  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e33c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2.2 KiB/2.2 KiB (22.7 KiB/s) with 1 file(s) remaining\r",
      "upload: ./constraints.json to s3://sagemaker-project-p-wh804xvsqt0r/Mlops-Titatnic-RnD-p-wh804xvsqt0r/qhxq0kmetdra/dataqualitycheckstep/constraints.json\r\n"
     ]
    }
   ],
   "source": [
    "#! aws s3 cp s3://sagemaker-project-p-wh804xvsqt0r/Mlops-Titatnic-RnD-p-wh804xvsqt0r/qhxq0kmetdra/dataqualitycheckstep/constraints.json /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/constraints.json s3://sagemaker-project-p-wh804xvsqt0r/Mlops-Titatnic-RnD-p-wh804xvsqt0r/qhxq0kmetdra/dataqualitycheckstep/constraints.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08f64366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2.8 KiB/2.8 KiB (35.5 KiB/s) with 1 file(s) remaining\r",
      "upload: deployment/score_xg_boost_processing_script.py to s3://mlops-insurance/scripts/score/xg/score_xg_boost_processing_script.py\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/score_xg_boost_processing_script.py s3://mlops-insurance/scripts/score/xg/score_xg_boost_processing_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e55f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://mlops-insurance/deploymentfiles/monitor_config.json to deployment/monitor_config.json\n",
      "download: s3://mlops-insurance/deploymentfiles/AthenaDB-TableV1.yml to deployment/AthenaDB-TableV1.yml\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/monitor_config.json  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/AthenaDB-TableV1.yml  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "#! cp /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deploy_model_monitor.sh  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d165114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/WipCoe/MLOPsDemo\n",
      "download: s3://mlops-insurance/deploymentfiles/GluebookmarkCode.py to deployment/GluebookmarkCode.py\n",
      "download: s3://mlops-insurance/deploymentfiles/deploy-batchmonitor.sh to deployment/deploy-batchmonitor.sh\n",
      "download: s3://mlops-insurance/deploymentfiles/batchmonitor-config.json to deployment/config/batchmonitor-config.json\n",
      "download: s3://mlops-insurance/deploymentfiles/common-config.json to deployment/config/common-config.json\n",
      "download: s3://mlops-insurance/deploymentfiles/script_postprocessing_csv_jsonl.py to deployment/script_postprocessing_csv_jsonl.py\n",
      "download: s3://mlops-insurance/deploymentfiles/BatchMonit-cfnV1.yml to deployment/BatchMonit-cfnV1.yml\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "#! aws s3 cp s3://mlops-insurance/batchinferdata/insurance.csv /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/GluebookmarkCode.py  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/deploy-batchmonitor.sh  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/batchmonitor-config.json  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/config/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/common-config.json  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/config/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/script_postprocessing_csv_jsonl.py  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/\n",
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/BatchMonit-cfnV1.yml  /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp s3://mlops-insurance/deploymentfiles/insurance.csv s3://mlops-insurance/baseline/insurance.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1edc5351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 36.8 KiB/36.8 KiB (269.4 KiB/s) with 1 file(s) remaining\r",
      "upload: ./anomoldata.csv to s3://mlops-insurance/data/scoreinput/anomoldata.csv\r\n"
     ]
    }
   ],
   "source": [
    "#! aws s3 cp /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/deployment/score_xg_boost_processing_script.py s3://mlops-insurance/scripts/score/xg/score_xg_boost_processing_script.py\n",
    "#! aws s3 cp s3://mlops-insurance/deploymentfiles/insurance.csv /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp anomoldata.csv s3://mlops-insurance/data/scoreinput/anomoldata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c12ac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id            int64\n",
      "age           int64\n",
      "sex          object\n",
      "bmi         float64\n",
      "children      int64\n",
      "smoker       object\n",
      "region       object\n",
      "dtype: object\n",
      "            0         1         2    3    4    5    6    7    8    9    10\n",
      "0    -1.438764 -0.453320 -0.908614  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0\n",
      "1    -1.509965  0.509621 -0.078767  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
      "2    -0.797954  0.383307  1.580926  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
      "3    -0.441948 -1.305531 -0.908614  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
      "4    -0.513149 -0.292556 -0.908614  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
      "...        ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "1333  0.768473  0.050297  1.580926  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
      "1334 -1.509965  0.206139 -0.908614  1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0\n",
      "1335 -1.509965  1.014878 -0.908614  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
      "1336 -1.296362 -0.797813 -0.908614  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
      "1337  1.551686 -0.261388 -0.908614  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0\n",
      "\n",
      "[1338 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import tempfile\n",
    "from sklearn import preprocessing\n",
    "import glob\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import uuid\n",
    "import time\n",
    "import datetime\n",
    "runtime=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "unqstr=datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "feature_columns_names = [\n",
    "    \"Id\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"bmi\",\n",
    "    \"children\",\n",
    "    \"smoker\",\n",
    "    \"region\",\n",
    "]\n",
    "feature_columns_dtype = {\n",
    "    \"Id\":str,\n",
    "    \"age\": np.float64,\n",
    "    \"sex\": str,\n",
    "    \"bmi\": np.float64,\n",
    "    \"children\": np.float64,\n",
    "    \"smoker\": str,\n",
    "    \"region\": str,\n",
    "}\n",
    "chunksize = 10000\n",
    "#path = r'/opt/ml/processing/input' # Input path\n",
    "path=\"/home/ec2-user/SageMaker/WipCoe/MLOPsDemo/anomoldata.csv\"\n",
    "#all_files = glob.glob(path + \"/*.csv\")\n",
    "#read them into pandas\n",
    "df_list =pd.read_csv(path,nrows=100000)\n",
    "#data = pd.concat(df_list)\n",
    "data = df_list\n",
    "df = data.copy()\n",
    "print(df.dtypes)\n",
    "#logger.debug(\"Defining transformers.\")\n",
    "numeric_features = list(feature_columns_names)\n",
    "numeric_features.remove(\"sex\")\n",
    "numeric_features.remove(\"smoker\")\n",
    "numeric_features.remove(\"region\")\n",
    "numeric_features.remove(\"Id\")\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_features = [\"sex\", \"smoker\", \"region\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "#logger.info(\"Applying transforms.\")\n",
    "#print(df.head(10))\n",
    "prepro_df = preprocess.fit_transform(df)\n",
    "#print(encoded_df.shape)\n",
    "encoded_df=pd.DataFrame(prepro_df)\n",
    "print(encoded_df)\n",
    "encoded_df['runtime']=runtime\n",
    "encoded_df['modelname']='XGboost'\n",
    "encoded_df['infertype']='Batch'\n",
    "encoded_df['id']=df.Id.apply(lambda x:str(x)+unqstr)\n",
    "encoded_df.insert(0,'runtime',encoded_df.pop('runtime'))\n",
    "encoded_df.insert(1,'modelname',encoded_df.pop('modelname'))\n",
    "encoded_df.insert(2,'infertype',encoded_df.pop('infertype'))\n",
    "encoded_df.insert(3,'id',encoded_df.pop('id'))\n",
    "#encoded_df.insert(4,'scorefilename',encoded_df.pop('scorefilename'))\n",
    "encoded_df.to_csv(\"preprocessdata.csv\",index=False, header=False)\n",
    "df['Id']=df.Id.apply(lambda x:str(x)+unqstr)\n",
    "df.to_csv(\"origdata.csv\",index=False,header=True)\n",
    "#encoded_df.to_csv(\"/opt/ml/processing/output/test/\"+filname+\"_{}.csv\".format(uuid.uuid1().time_low), index=False, header=False) # test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "307f0ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.020230307050840\n",
       "1       2.020230307050840\n",
       "2       3.020230307050840\n",
       "3       4.020230307050840\n",
       "4       5.020230307050840\n",
       "              ...        \n",
       "1333    nan20230307050840\n",
       "1334    nan20230307050840\n",
       "1335    nan20230307050840\n",
       "1336    nan20230307050840\n",
       "1337    nan20230307050840\n",
       "Name: Id, Length: 1338, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b2310d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-42e0510d53fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#read them into pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "## pre processing script\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import tempfile\n",
    "from sklearn import preprocessing\n",
    "import glob\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import uuid\n",
    "import time\n",
    "import datetime\n",
    "runtime=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "unqstr=datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "feature_columns_names = [\n",
    "    \"Id\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"bmi\",\n",
    "    \"children\",\n",
    "    \"smoker\",\n",
    "    \"region\",\n",
    "]\n",
    "feature_columns_dtype = {\n",
    "    \"Id\":str,\n",
    "    \"age\": np.float64,\n",
    "    \"sex\": str,\n",
    "    \"bmi\": np.float64,\n",
    "    \"children\": np.float64,\n",
    "    \"smoker\": str,\n",
    "    \"region\": str,\n",
    "}\n",
    "chunksize = 10000\n",
    "#path = r'/opt/ml/processing/input' # Input path\n",
    "path=\"/home/ec2-user/SageMaker/WipCoe/MLOPsDemo/insurance.csv\"\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "#read them into pandas\n",
    "df_list = [pd.read_csv(filename,nrows=100000) for filename in all_files]\n",
    "data = [pd.concat(df_list)\n",
    "data = df_list\n",
    "df = data.copy()\n",
    "print(df.dtypes)\n",
    "#logger.debug(\"Defining transformers.\")\n",
    "numeric_features = list(feature_columns_names)\n",
    "numeric_features.remove(\"sex\")\n",
    "numeric_features.remove(\"smoker\")\n",
    "numeric_features.remove(\"region\")\n",
    "numeric_features.remove(\"Id\")\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_features = [\"sex\", \"smoker\", \"region\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "#logger.info(\"Applying transforms.\")\n",
    "#print(df.head(10))\n",
    "prepro_df = preprocess.fit_transform(df)\n",
    "print(encoded_df.shape)\n",
    "encoded_df=pd.DataFrame(prepro_df)\n",
    "print(encoded_df)\n",
    "encoded_df['runtime']=runtime\n",
    "encoded_df['modelname']='XGboost'\n",
    "encoded_df['infertype']='Batch'\n",
    "encoded_df['id']=df.Id.apply(lambda x:str(x)+unqstr)\n",
    "encoded_df.insert(0,'runtime',encoded_df.pop('runtime'))\n",
    "encoded_df.insert(1,'modelname',encoded_df.pop('modelname'))\n",
    "encoded_df.insert(2,'infertype',encoded_df.pop('infertype'))\n",
    "encoded_df.insert(3,'id',encoded_df.pop('id'))\n",
    "#encoded_df.insert(4,'scorefilename',encoded_df.pop('scorefilename'))\n",
    "encoded_df.to_csv(\"preprocessdata.csv\",index=False, header=False)\n",
    "#encoded_df.to_csv(\"/opt/ml/processing/output/test/\"+filname+\"_{}.csv\".format(uuid.uuid1().time_low), index=False, header=False) # test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25044e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3761ba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ec2-user/SageMaker/WipCoe/MLOPsDemo/insurance.csv']\n",
      "       Id  age     sex     bmi  children smoker     region\n",
      "1       2   28    male  3.3770         1     no  southeast\n",
      "2       3   38    male  3.3000         3     no  southeast\n",
      "3       4   43    male  2.2705         0     no  northwest\n",
      "4       5   42    male  2.8880         0     no  northwest\n",
      "5       6   41  female  2.5740         0     no  southeast\n",
      "..    ...  ...     ...     ...       ...    ...        ...\n",
      "995   996   49  female  2.3275         3     no  northeast\n",
      "996   997   49  female  3.4100         3     no  southwest\n",
      "997   998   73  female  3.6850         0     no  southeast\n",
      "998   999   43  female  3.6290         3     no  northeast\n",
      "999  1000   46  female  2.6885         0     no  northwest\n",
      "\n",
      "[999 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/ec2-user/SageMaker/WipCoe/MLOPsDemo/insurance.csv\"\n",
    "all_files = glob.glob(path)\n",
    "print(all_files)\n",
    "df_list = [pd.read_csv(filename,nrows=100000) for filename in all_files]\n",
    "data = pd.concat(df_list)\n",
    "data['age']=data['age']+10\n",
    "data['bmi']=data['bmi']/10\n",
    "print(data[1:1000])\n",
    "data[1:1000].to_csv(\"anomoldata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cb7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://mlops-insurance/scoring/output/batch/xgresult/2023/03/07/11/batchinsurance_679889326.csv.out to ./batchinsurance_679889326.csv.out\n",
      "download: s3://mlops-insurance/scoring/output/batch/xgresult/2023/03/07/11/batchinsurance_2224405814.csv.out to ./batchinsurance_2224405814.csv.out\n",
      "download: s3://mlops-insurance/transformed/insurdemo/monitoring/inbound/currentrun/batch/xg/run-1678351380072-part-r-00001 to ./run-1678351380072-part-r-00001\n",
      "download: s3://mlops-insurance/transformed/insurdemo/monitoring/inbound/currentrun/batch/xg/run-1678351380072-part-r-00000 to ./run-1678351380072-part-r-00000\n",
      "download: s3://mlops-insurance/transformed/insurdemo/monitoring/inbound/currentrun/batch/xg/run-1678351380072-part-r-00002 to ./run-1678351380072-part-r-00002\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://mlops-insurance/scoring/output/batch/xgresult/2023/03/07/11/batchinsurance_679889326.csv.out /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp s3://mlops-insurance/scoring/output/batch/xgresult/2023/03/07/11/batchinsurance_2224405814.csv.out /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/  \n",
    "! aws s3 cp s3://mlops-insurance/transformed/insurdemo/monitoring/inbound/currentrun/batch/xg/run-1678351380072-part-r-00001   /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp s3://mlops-insurance/transformed/insurdemo/monitoring/inbound/currentrun/batch/xg/run-1678351380072-part-r-00000   /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "! aws s3 cp s3://mlops-insurance/transformed/insurdemo/monitoring/inbound/currentrun/batch/xg/run-1678351380072-part-r-00002   /home/ec2-user/SageMaker/WipCoe/MLOPsDemo/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe4457b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run-1678351380072-part-r-00001', 'run-1678351380072-part-r-00000', 'run-1678351380072-part-r-00002']\n",
      "hi\n",
      "datacapture.jsonl\n",
      "datacapture.jsonl\n",
      "hi\n",
      "datacapture.jsonl\n",
      "datacapture.jsonl\n",
      "hi\n",
      "datacapture.jsonl\n",
      "datacapture.jsonl\n"
     ]
    }
   ],
   "source": [
    "#@csv to Jsonl\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "#path = r'/opt/ml/processing/input' # Input path\n",
    "#path='/opt/ml/processing/input/data'\n",
    "#all_files = glob.glob(path + \"/*\",recursive=True)\n",
    "all_files=['run-1678351380072-part-r-00001','run-1678351380072-part-r-00000','run-1678351380072-part-r-00002']\n",
    "counter = 0\n",
    "print(all_files)\n",
    "for filename in all_files:\n",
    "    print(\"hi\")\n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    df = df.sample(frac=.25)\n",
    "    df=df.iloc[: ,3:].drop([4],axis = 1)\n",
    "    # Create a multiline json\n",
    "    json_list = json.loads(df.to_json(orient = \"records\"))\n",
    "    output_path = \"datacapture.jsonl\" #path to the linear learner\n",
    "    print(output_path)\n",
    "    counter = counter + 1\n",
    "    data = {}\n",
    "    data[\"captureData\"]={\n",
    "            \"endpointInput\": {\n",
    "                \"observedContentType\": \"text/csv\",\n",
    "                \"mode\": \"INPUT\",\n",
    "                \"data\": \"132,25,113.2,96,269.9,107,229.1,87,7.1,7,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1\",\n",
    "                \"encoding\": \"CSV\"\n",
    "            },\n",
    "            \"endpointOutput\": {\n",
    "                \"observedContentType\": \"text/csv; charset=utf-8\",\n",
    "                \"mode\": \"OUTPUT\",\n",
    "                \"data\": \"6295.23876953125\",\n",
    "                \"encoding\": \"CSV\"\n",
    "            }\n",
    "        }\n",
    "    data[\"eventMetadata\"] = {\n",
    "            \"eventId\": \"\",\n",
    "            \"inferenceTime\": \"2\"\n",
    "        }\n",
    "    data[\"eventVersion\"] = \"0\"\n",
    "    i=0\n",
    "    with open(output_path, 'w') as f:\n",
    "        print(output_path)\n",
    "        for item in json_list:\n",
    "            i=i+1\n",
    "            if item['5'][0:3]!='col':\n",
    "                item = list(item.values())\n",
    "                inpitem = ','.join([str(elem) for elem in item[1:-1]])\n",
    "                #outitem = ','.join([str(elem) for elem in item[-1]])\n",
    "                outitem=str(item[-1])\n",
    "                data[\"captureData\"][\"endpointInput\"][\"data\"] = inpitem\n",
    "                data[\"captureData\"][\"endpointOutput\"][\"data\"] =outitem\n",
    "                data[\"eventMetadata\"][\"eventId\"]=str(item[0])\n",
    "                if i==len(json_list):\n",
    "                    f.write(\"%s\" % data)\n",
    "                else:\n",
    "                    f.write(\"%s\\n\" % data)\n",
    "# Data push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a359235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "abc='123'\n",
    "print((abc[0:3]!='col'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d369280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"MonitoringScheduleSummaries\": [\r\n",
      "        {\r\n",
      "            \"MonitoringScheduleName\": \"Mlops-Titatnic-RnD-staging-ModelExplainability\",\r\n",
      "            \"MonitoringScheduleArn\": \"arn:aws:sagemaker:us-east-1:525102048888:monitoring-schedule/mlops-titatnic-rnd-staging-modelexplainability\",\r\n",
      "            \"CreationTime\": 1678777109.274,\r\n",
      "            \"LastModifiedTime\": 1678900279.505,\r\n",
      "            \"MonitoringScheduleStatus\": \"Scheduled\",\r\n",
      "            \"EndpointName\": \"Mlops-Titatnic-RnD-staging\",\r\n",
      "            \"MonitoringJobDefinitionName\": \"cfn-modelexplainabilityj-veuaxb9kxteq\",\r\n",
      "            \"MonitoringType\": \"ModelExplainability\"\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"MonitoringScheduleName\": \"Mlops-Titatnic-RnD-staging-DataQuality\",\r\n",
      "            \"MonitoringScheduleArn\": \"arn:aws:sagemaker:us-east-1:525102048888:monitoring-schedule/mlops-titatnic-rnd-staging-dataquality\",\r\n",
      "            \"CreationTime\": 1678777109.249,\r\n",
      "            \"LastModifiedTime\": 1678900154.612,\r\n",
      "            \"MonitoringScheduleStatus\": \"Scheduled\",\r\n",
      "            \"EndpointName\": \"Mlops-Titatnic-RnD-staging\",\r\n",
      "            \"MonitoringJobDefinitionName\": \"cfn-dataqualityjobdefini-99dyci9zsy06\",\r\n",
      "            \"MonitoringType\": \"DataQuality\"\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! aws sagemaker delete-monitoring-schedule\n",
    "\n",
    "! aws sagemaker list-monitoring-schedules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
